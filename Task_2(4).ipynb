{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h3VYlwgJvZql"
      },
      "outputs": [],
      "source": [
        "# !pip install torch torchvision --quiet\n",
        "\n",
        "import os, math, time, random\n",
        "from dataclasses import dataclass\n",
        "from typing import Tuple, Dict, Any, List\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.utils.data import DataLoader, Subset, random_split\n",
        "from torchvision import transforms, datasets, models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jelMLlFGv-Jw"
      },
      "outputs": [],
      "source": [
        "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "SEED = 42\n",
        "random.seed(SEED); torch.manual_seed(SEED); torch.cuda.manual_seed_all(SEED)\n",
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "NUM_CLASSES = 10  # STL-10 has 10 classes\n",
        "IMG_SIZE = 224    # upscale 96 -> 224 for ResNet-152\n",
        "BATCH_SIZE = 64   # reduce if memory-limited\n",
        "NUM_WORKERS = 4   # bump if you have CPU headroom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZHOkPFlWvqrI",
        "outputId": "d4c06cbe-b193-4f9e-8cb6-9426604f6ada"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2.64G/2.64G [01:00<00:00, 43.4MB/s]\n",
            "/usr/local/lib/python3.12/dist-packages/torch/utils/data/dataloader.py:627: UserWarning: This DataLoader will create 4 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "\n",
        "\n",
        "# ---------- Transforms ----------\n",
        "# Use ImageNet normalization because we rely on ImageNet-pretrained weights\n",
        "mean = [0.485, 0.456, 0.406]\n",
        "std  = [0.229, 0.224, 0.225]\n",
        "train_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.RandomResizedCrop(IMG_SIZE, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(p=0.5),\n",
        "    transforms.ColorJitter(0.1, 0.1, 0.1, 0.05),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "eval_tfms = transforms.Compose([\n",
        "    transforms.Resize((IMG_SIZE, IMG_SIZE)),\n",
        "    transforms.CenterCrop(IMG_SIZE),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean, std),\n",
        "])\n",
        "\n",
        "# ---------- Data ----------\n",
        "def get_stl10_dataloaders(data_root=\"./data\", val_ratio=0.2)->Tuple[DataLoader, DataLoader, DataLoader]:\n",
        "    # STL10 train split has 5k labeled images; test has 8k\n",
        "    train_full = datasets.STL10(root=data_root, split='train', download=True, transform=train_tfms)\n",
        "    test_set   = datasets.STL10(root=data_root, split='test',  download=True, transform=eval_tfms)\n",
        "\n",
        "    val_size = int(len(train_full) * val_ratio)\n",
        "    train_size = len(train_full) - val_size\n",
        "    g = torch.Generator().manual_seed(SEED)\n",
        "    train_set, val_set = random_split(train_full, [train_size, val_size], generator=g)\n",
        "\n",
        "    train_loader = DataLoader(train_set, batch_size=BATCH_SIZE, shuffle=True,  num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    val_loader   = DataLoader(val_set,   batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    test_loader  = DataLoader(test_set,  batch_size=BATCH_SIZE, shuffle=False, num_workers=NUM_WORKERS, pin_memory=True)\n",
        "    return train_loader, val_loader, test_loader\n",
        "\n",
        "train_loader, val_loader, test_loader = get_stl10_dataloaders()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2xA_Ts4-wNbD"
      },
      "outputs": [],
      "source": [
        "# ---------- Model helpers ----------\n",
        "def make_resnet152(pretrained: bool, num_classes: int = NUM_CLASSES) -> nn.Module:\n",
        "    if pretrained:\n",
        "        weights = models.ResNet152_Weights.IMAGENET1K_V2\n",
        "        model = models.resnet152(weights=weights)\n",
        "    else:\n",
        "        model = models.resnet152(weights=None)\n",
        "\n",
        "    in_feats = model.fc.in_features\n",
        "    model.fc = nn.Linear(in_feats, num_classes)  # new head\n",
        "    return model\n",
        "\n",
        "def freeze_all(m: nn.Module):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = False\n",
        "\n",
        "def unfreeze_all(m: nn.Module):\n",
        "    for p in m.parameters():\n",
        "        p.requires_grad = True\n",
        "\n",
        "def trainable_params(m: nn.Module):\n",
        "    return [p for p in m.parameters() if p.requires_grad]\n",
        "\n",
        "# ---------- Optimizers / schedulers ----------\n",
        "def make_optimizer(model, lr_backbone, lr_head=None, wd=1e-4, last_block_only=False, full_ft=False):\n",
        "    \"\"\"\n",
        "    - head-only: call with lr_backbone=None and pass only head params externally\n",
        "    - last_block_only: layer4 gets lr_backbone; fc gets lr_head\n",
        "    - full_ft: backbone gets lr_backbone; fc gets lr_head\n",
        "    \"\"\"\n",
        "    params = []\n",
        "    if last_block_only:\n",
        "        assert lr_backbone is not None and lr_head is not None\n",
        "        params.append({\"params\": model.layer4.parameters(), \"lr\": lr_backbone})\n",
        "        params.append({\"params\": model.fc.parameters(),     \"lr\": lr_head})\n",
        "    elif full_ft:\n",
        "        assert lr_backbone is not None and lr_head is not None\n",
        "        # backbone: everything except fc\n",
        "        backbone_params = []\n",
        "        for n, p in model.named_parameters():\n",
        "            if p.requires_grad and not n.startswith(\"fc.\"):\n",
        "                backbone_params.append(p)\n",
        "        params.append({\"params\": backbone_params, \"lr\": lr_backbone})\n",
        "        params.append({\"params\": model.fc.parameters(), \"lr\": lr_head})\n",
        "    else:\n",
        "        # head-only handled outside (just pass model.fc.parameters()).\n",
        "        pass\n",
        "\n",
        "    opt = torch.optim.AdamW(params if params else model.fc.parameters(), lr=lr_head if lr_head else lr_backbone, weight_decay=wd)\n",
        "    return opt\n",
        "\n",
        "def make_cosine_scheduler(optimizer, warmup_steps, total_steps):\n",
        "    def lr_lambda(step):\n",
        "        if step < warmup_steps:\n",
        "            return float(step) / float(max(1, warmup_steps))\n",
        "        progress = float(step - warmup_steps) / float(max(1, total_steps - warmup_steps))\n",
        "        return 0.5 * (1.0 + math.cos(math.pi * progress))\n",
        "    return torch.optim.lr_scheduler.LambdaLR(optimizer, lr_lambda)\n",
        "\n",
        "# ---------- Training / Eval ----------\n",
        "@dataclass\n",
        "class TrainConfig:\n",
        "    epochs: int = 20\n",
        "    label_smoothing: float = 0.0\n",
        "    early_stop_patience: int = 3\n",
        "    max_steps_per_epoch: int = None  # set to int to cap steps for quick debug\n",
        "    use_amp: bool = True\n",
        "\n",
        "class EarlyStopper:\n",
        "    def __init__(self, patience=3, mode='max'):\n",
        "        self.patience = patience\n",
        "        self.mode = mode\n",
        "        self.best = -float('inf') if mode=='max' else float('inf')\n",
        "        self.bad_epochs = 0\n",
        "        self.should_stop = False\n",
        "\n",
        "    def step(self, metric):\n",
        "        improved = (metric > self.best) if self.mode=='max' else (metric < self.best)\n",
        "        if improved:\n",
        "            self.best = metric\n",
        "            self.bad_epochs = 0\n",
        "        else:\n",
        "            self.bad_epochs += 1\n",
        "            if self.bad_epochs >= self.patience:\n",
        "                self.should_stop = True\n",
        "\n",
        "def accuracy(logits, y):\n",
        "    preds = logits.argmax(1)\n",
        "    return (preds == y).float().mean().item()\n",
        "\n",
        "def evaluate(model, loader, criterion):\n",
        "    model.eval()\n",
        "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
        "    with torch.no_grad():\n",
        "        for x, y in loader:\n",
        "            x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            bs = y.size(0)\n",
        "            total_loss += loss.item() * bs\n",
        "            total_acc  += accuracy(logits, y) * bs\n",
        "            total_n    += bs\n",
        "    return total_loss/total_n, total_acc/total_n\n",
        "\n",
        "def train_one_epoch(model, loader, criterion, optimizer, scaler: torch.cuda.amp.GradScaler=None, max_steps=None):\n",
        "    model.train()\n",
        "    total_loss, total_acc, total_n = 0.0, 0.0, 0\n",
        "    step = 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
        "        optimizer.zero_grad(set_to_none=True)\n",
        "        if scaler is not None:\n",
        "            with torch.cuda.amp.autocast():\n",
        "                logits = model(x)\n",
        "                loss = criterion(logits, y)\n",
        "            scaler.scale(loss).backward()\n",
        "            scaler.step(optimizer)\n",
        "            scaler.update()\n",
        "        else:\n",
        "            logits = model(x)\n",
        "            loss = criterion(logits, y)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        bs = y.size(0)\n",
        "        total_loss += loss.item() * bs\n",
        "        total_acc  += accuracy(logits, y) * bs\n",
        "        total_n    += bs\n",
        "        step += 1\n",
        "        if max_steps and step >= max_steps:\n",
        "            break\n",
        "    return total_loss/total_n, total_acc/total_n\n",
        "\n",
        "def fit(model, train_loader, val_loader, optimizer, scheduler, config: TrainConfig):\n",
        "    model.to(DEVICE)\n",
        "    criterion = nn.CrossEntropyLoss(label_smoothing=config.label_smoothing)\n",
        "    scaler = torch.cuda.amp.GradScaler() if (config.use_amp and DEVICE.type=='cuda') else None\n",
        "\n",
        "    stopper = EarlyStopper(patience=config.early_stop_patience, mode='max')\n",
        "    best = {\"epoch\": -1, \"val_acc\": -1.0, \"state_dict\": None}\n",
        "\n",
        "    global_step = 0\n",
        "    total_steps = config.epochs * len(train_loader)\n",
        "    warmup = max(1, int(0.05 * total_steps))  # ~5% warmup; tweak per taste\n",
        "\n",
        "    start = time.time()\n",
        "    for epoch in range(1, config.epochs+1):\n",
        "        tr_loss, tr_acc = train_one_epoch(model, train_loader, criterion, optimizer, scaler, max_steps=config.max_steps_per_epoch)\n",
        "        val_loss, val_acc = evaluate(model, val_loader, criterion)\n",
        "\n",
        "        # Update scheduler per step (cosine lambda depends on step)\n",
        "        if scheduler:\n",
        "            # Advance by number of batches seen this epoch\n",
        "            for _ in range(len(train_loader if not config.max_steps_per_epoch else range(config.max_steps_per_epoch))):\n",
        "                scheduler.step()\n",
        "                global_step += 1\n",
        "\n",
        "        print(f\"[Epoch {epoch:02d}] train loss={tr_loss:.4f} acc={tr_acc*100:.2f}% | val loss={val_loss:.4f} acc={val_acc*100:.2f}%\")\n",
        "\n",
        "        if val_acc > best[\"val_acc\"]:\n",
        "            best.update({\"epoch\": epoch, \"val_acc\": val_acc, \"state_dict\": {k:v.cpu() for k,v in model.state_dict().items()}})\n",
        "\n",
        "        stopper.step(val_acc)\n",
        "        if stopper.should_stop:\n",
        "            print(\"Early stopping.\")\n",
        "            break\n",
        "\n",
        "    took = time.time() - start\n",
        "    print(f\"Best @ epoch {best['epoch']}: val acc={best['val_acc']*100:.2f}% | time={took/60:.1f} min\")\n",
        "    # load best weights back to model\n",
        "    if best[\"state_dict\"] is not None:\n",
        "        model.load_state_dict({k:v.to(DEVICE) for k,v in best[\"state_dict\"].items()})\n",
        "    return best"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ql7_J_BXwtpr",
        "outputId": "c559acb0-d765-4bff-f966-e4c584177239"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet152-f82ba261.pth\" to /root/.cache/torch/hub/checkpoints/resnet152-f82ba261.pth\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 230M/230M [00:01<00:00, 163MB/s]\n",
            "/tmp/ipython-input-3529283715.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if (config.use_amp and DEVICE.type=='cuda') else None\n",
            "/tmp/ipython-input-3529283715.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
            "/tmp/ipython-input-3529283715.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3529283715.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] train loss=2.2818 acc=15.00% | val loss=2.2882 acc=12.40%\n",
            "[Epoch 02] train loss=1.6920 acc=70.67% | val loss=1.2652 acc=90.70%\n",
            "[Epoch 03] train loss=0.8545 acc=94.27% | val loss=0.6761 acc=94.20%\n",
            "[Epoch 04] train loss=0.4893 acc=95.97% | val loss=0.4645 acc=95.30%\n",
            "[Epoch 05] train loss=0.3517 acc=96.38% | val loss=0.3580 acc=95.70%\n",
            "[Epoch 06] train loss=0.2799 acc=96.65% | val loss=0.3149 acc=95.60%\n",
            "[Epoch 07] train loss=0.2351 acc=97.42% | val loss=0.2630 acc=96.40%\n",
            "[Epoch 08] train loss=0.2061 acc=97.50% | val loss=0.2402 acc=96.70%\n",
            "[Epoch 09] train loss=0.1850 acc=97.72% | val loss=0.2373 acc=95.90%\n",
            "[Epoch 10] train loss=0.1719 acc=97.90% | val loss=0.2187 acc=96.60%\n",
            "[Epoch 11] train loss=0.1693 acc=97.47% | val loss=0.2091 acc=96.30%\n",
            "Early stopping.\n",
            "Best @ epoch 8: val acc=96.70% | time=7.4 min\n",
            "E1 Test acc: 97.22%\n"
          ]
        }
      ],
      "source": [
        "# E1: Pretrained backbone frozen, train only the new head\n",
        "config = TrainConfig(epochs=15, label_smoothing=0.0, early_stop_patience=3, use_amp=True)\n",
        "\n",
        "model_e1 = make_resnet152(pretrained=True, num_classes=NUM_CLASSES)\n",
        "freeze_all(model_e1)                # freeze everything\n",
        "for p in model_e1.fc.parameters():  # unfreeze head\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Optimizer only on head\n",
        "opt_e1 = torch.optim.AdamW(model_e1.fc.parameters(), lr=5e-4, weight_decay=1e-4)\n",
        "# Cosine scheduler over (epochs * steps); will be advanced inside fit()\n",
        "sched_e1 = make_cosine_scheduler(opt_e1, warmup_steps=100, total_steps=config.epochs*len(train_loader))\n",
        "\n",
        "best_e1 = fit(model_e1, train_loader, val_loader, opt_e1, sched_e1, config)\n",
        "\n",
        "# Evaluate on test set\n",
        "test_loss, test_acc = evaluate(model_e1, test_loader, nn.CrossEntropyLoss())\n",
        "print(f\"E1 Test acc: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eBB7KeLbzc_k"
      },
      "outputs": [],
      "source": [
        "torch.save(best_e1[\"state_dict\"], \"resnet152_E1_head_only.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BOgwMtb2xAMa",
        "outputId": "bd32b93b-6769-42be-c7a4-b2f256f17038"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3529283715.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if (config.use_amp and DEVICE.type=='cuda') else None\n",
            "/tmp/ipython-input-3529283715.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
            "/tmp/ipython-input-3529283715.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3529283715.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] train loss=2.2891 acc=12.25% | val loss=2.2917 acc=11.90%\n",
            "[Epoch 02] train loss=1.5100 acc=70.70% | val loss=0.5353 acc=95.20%\n",
            "[Epoch 03] train loss=0.1969 acc=96.43% | val loss=0.0983 acc=97.40%\n",
            "[Epoch 04] train loss=0.0750 acc=97.95% | val loss=0.0772 acc=97.50%\n",
            "[Epoch 05] train loss=0.0404 acc=98.83% | val loss=0.0965 acc=97.10%\n",
            "[Epoch 06] train loss=0.0262 acc=99.28% | val loss=0.0722 acc=97.90%\n",
            "[Epoch 07] train loss=0.0220 acc=99.35% | val loss=0.0928 acc=97.30%\n",
            "[Epoch 08] train loss=0.0181 acc=99.45% | val loss=0.0734 acc=98.00%\n",
            "[Epoch 09] train loss=0.0211 acc=99.42% | val loss=0.0694 acc=97.80%\n",
            "[Epoch 10] train loss=0.0160 acc=99.72% | val loss=0.0787 acc=97.60%\n",
            "[Epoch 11] train loss=0.0132 acc=99.60% | val loss=0.0945 acc=97.00%\n",
            "Early stopping.\n",
            "Best @ epoch 8: val acc=98.00% | time=7.6 min\n",
            "E2 Test acc: 98.29%\n"
          ]
        }
      ],
      "source": [
        "# E2: Pretrained; unfreeze layer4 + head (discriminative LRs)\n",
        "config = TrainConfig(epochs=20, label_smoothing=0.0, early_stop_patience=3, use_amp=True)\n",
        "\n",
        "model_e2 = make_resnet152(pretrained=True, num_classes=NUM_CLASSES)\n",
        "freeze_all(model_e2)\n",
        "for p in model_e2.layer4.parameters():\n",
        "    p.requires_grad = True\n",
        "for p in model_e2.fc.parameters():\n",
        "    p.requires_grad = True\n",
        "\n",
        "# Discriminative LRs: smaller for backbone block, larger for head\n",
        "opt_e2 = make_optimizer(model_e2, lr_backbone=1e-4, lr_head=5e-4, wd=1e-4, last_block_only=True)\n",
        "sched_e2 = make_cosine_scheduler(opt_e2, warmup_steps=200, total_steps=config.epochs*len(train_loader))\n",
        "\n",
        "best_e2 = fit(model_e2, train_loader, val_loader, opt_e2, sched_e2, config)\n",
        "\n",
        "test_loss, test_acc = evaluate(model_e2, test_loader, nn.CrossEntropyLoss())\n",
        "print(f\"E2 Test acc: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f8WKQJaZzjnf"
      },
      "outputs": [],
      "source": [
        "torch.save(best_e2[\"state_dict\"], \"resnet152_E2_lastblock.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "83_lv1y9xIfA",
        "outputId": "2a33bc83-a81f-443f-b79a-e7684d34fa6c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3529283715.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if (config.use_amp and DEVICE.type=='cuda') else None\n",
            "/tmp/ipython-input-3529283715.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
            "/tmp/ipython-input-3529283715.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3529283715.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] train loss=2.3686 acc=5.67% | val loss=2.3649 acc=5.60%\n",
            "[Epoch 02] train loss=2.2835 acc=14.65% | val loss=2.2021 acc=22.60%\n",
            "[Epoch 03] train loss=1.9658 acc=49.33% | val loss=1.6860 acc=69.80%\n",
            "[Epoch 04] train loss=1.0933 acc=85.97% | val loss=0.5386 acc=95.30%\n",
            "[Epoch 05] train loss=0.2554 acc=97.15% | val loss=0.1423 acc=97.10%\n",
            "[Epoch 06] train loss=0.0824 acc=98.40% | val loss=0.0940 acc=97.70%\n",
            "[Epoch 07] train loss=0.0458 acc=99.15% | val loss=0.0689 acc=98.10%\n",
            "[Epoch 08] train loss=0.0267 acc=99.40% | val loss=0.0733 acc=98.00%\n",
            "[Epoch 09] train loss=0.0187 acc=99.70% | val loss=0.0561 acc=98.10%\n",
            "[Epoch 10] train loss=0.0184 acc=99.67% | val loss=0.0619 acc=97.50%\n",
            "Early stopping.\n",
            "Best @ epoch 7: val acc=98.10% | time=8.4 min\n",
            "E3 Test acc: 98.52%\n"
          ]
        }
      ],
      "source": [
        "# E3: Pretrained; unfreeze everything, tiny LR for backbone, larger for head\n",
        "config = TrainConfig(epochs=20, label_smoothing=0.0, early_stop_patience=3, use_amp=True)\n",
        "\n",
        "model_e3 = make_resnet152(pretrained=True, num_classes=NUM_CLASSES)\n",
        "unfreeze_all(model_e3)\n",
        "\n",
        "# Gentle LRs to avoid destroying pretrained features\n",
        "opt_e3 = make_optimizer(model_e3, lr_backbone=2e-5, lr_head=8e-5, wd=1e-4, full_ft=True)\n",
        "sched_e3 = make_cosine_scheduler(opt_e3, warmup_steps=300, total_steps=config.epochs*len(train_loader))\n",
        "\n",
        "best_e3 = fit(model_e3, train_loader, val_loader, opt_e3, sched_e3, config)\n",
        "\n",
        "test_loss, test_acc = evaluate(model_e3, test_loader, nn.CrossEntropyLoss())\n",
        "print(f\"E3 Test acc: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eegva8zjzmWS"
      },
      "outputs": [],
      "source": [
        "torch.save(best_e3[\"state_dict\"], \"resnet152_E3_full_pretrained.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "background_save": true,
          "base_uri": "https://localhost:8080/"
        },
        "id": "0UU5p8bHxKsy",
        "outputId": "43611531-5a7f-40ed-c531-8b6987d03e97"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3529283715.py:137: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
            "  scaler = torch.cuda.amp.GradScaler() if (config.use_amp and DEVICE.type=='cuda') else None\n",
            "/tmp/ipython-input-3529283715.py:110: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
            "/tmp/ipython-input-3529283715.py:113: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
            "  with torch.cuda.amp.autocast():\n",
            "/tmp/ipython-input-3529283715.py:96: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.detach().clone() or sourceTensor.detach().clone().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[Epoch 01] train loss=2.5049 acc=9.95% | val loss=2.4215 acc=10.20%\n",
            "[Epoch 02] train loss=2.3597 acc=13.75% | val loss=2.3239 acc=19.50%\n",
            "[Epoch 03] train loss=2.1495 acc=21.82% | val loss=2.1757 acc=22.80%\n",
            "[Epoch 04] train loss=2.0400 acc=27.38% | val loss=2.0241 acc=28.90%\n",
            "[Epoch 05] train loss=1.9129 acc=31.10% | val loss=1.9409 acc=32.80%\n",
            "[Epoch 06] train loss=1.8931 acc=33.42% | val loss=1.9021 acc=35.90%\n",
            "[Epoch 07] train loss=1.8375 acc=35.15% | val loss=1.9737 acc=35.40%\n",
            "[Epoch 08] train loss=1.8171 acc=37.62% | val loss=1.8728 acc=38.00%\n",
            "[Epoch 09] train loss=1.7657 acc=39.57% | val loss=1.7218 acc=41.30%\n",
            "[Epoch 10] train loss=1.7228 acc=42.52% | val loss=1.6879 acc=41.90%\n",
            "[Epoch 11] train loss=1.6531 acc=45.12% | val loss=1.7459 acc=41.00%\n",
            "[Epoch 12] train loss=1.6090 acc=48.20% | val loss=1.6626 acc=44.30%\n",
            "[Epoch 13] train loss=1.5732 acc=49.48% | val loss=1.7847 acc=40.40%\n",
            "[Epoch 14] train loss=1.5322 acc=52.30% | val loss=1.6622 acc=48.30%\n",
            "[Epoch 15] train loss=1.4648 acc=55.25% | val loss=1.7284 acc=47.20%\n",
            "[Epoch 16] train loss=1.4587 acc=56.38% | val loss=1.5990 acc=51.30%\n",
            "[Epoch 17] train loss=1.3722 acc=60.70% | val loss=3.0870 acc=46.90%\n",
            "[Epoch 18] train loss=1.3462 acc=62.12% | val loss=1.5313 acc=54.60%\n",
            "[Epoch 19] train loss=1.2974 acc=64.92% | val loss=1.5297 acc=55.00%\n",
            "[Epoch 20] train loss=1.2469 acc=67.15% | val loss=1.4466 acc=58.90%\n",
            "[Epoch 21] train loss=1.1922 acc=69.38% | val loss=1.4417 acc=57.80%\n",
            "[Epoch 22] train loss=1.1402 acc=72.35% | val loss=1.4427 acc=59.80%\n",
            "[Epoch 23] train loss=1.0923 acc=74.80% | val loss=1.5185 acc=59.60%\n",
            "[Epoch 24] train loss=1.0414 acc=77.65% | val loss=1.4420 acc=59.70%\n",
            "[Epoch 25] train loss=1.0135 acc=79.20% | val loss=1.4545 acc=60.80%\n",
            "Best @ epoch 25: val acc=60.80% | time=20.3 min\n",
            "E4 Test acc: 61.86%\n"
          ]
        }
      ],
      "source": [
        "# E4: Scratch init; full training. Stronger regularization/aug may help.\n",
        "config = TrainConfig(epochs=25, label_smoothing=0.1, early_stop_patience=4, use_amp=True)\n",
        "\n",
        "model_e4 = make_resnet152(pretrained=False, num_classes=NUM_CLASSES)\n",
        "unfreeze_all(model_e4)\n",
        "\n",
        "# Larger LR (scratch), maybe SGD with momentum also works well; here we use AdamW\n",
        "opt_e4 = torch.optim.AdamW(model_e4.parameters(), lr=1e-3, weight_decay=2e-4)\n",
        "sched_e4 = make_cosine_scheduler(opt_e4, warmup_steps=500, total_steps=config.epochs*len(train_loader))\n",
        "\n",
        "best_e4 = fit(model_e4, train_loader, val_loader, opt_e4, sched_e4, config)\n",
        "\n",
        "test_loss, test_acc = evaluate(model_e4, test_loader, nn.CrossEntropyLoss())\n",
        "print(f\"E4 Test acc: {test_acc*100:.2f}%\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 144
        },
        "id": "qVFMcJHezowh",
        "outputId": "3f02df6c-c454-4920-c67e-bf85dceaee52"
      },
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'torch' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-2610586463.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbest_e4\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"state_dict\"\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"resnet152_E4_full_scratch.pth\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m: name 'torch' is not defined"
          ]
        }
      ],
      "source": [
        "torch.save(best_e4[\"state_dict\"], \"resnet152_E4_full_scratch.pth\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5iqNBeLxM56"
      },
      "outputs": [],
      "source": [
        "def summarize(tag, best, model):\n",
        "    test_loss, test_acc = evaluate(model, test_loader, nn.CrossEntropyLoss())\n",
        "    return {\n",
        "        \"exp\": tag,\n",
        "        \"best_epoch\": best[\"epoch\"],\n",
        "        \"val_acc_best\": round(best[\"val_acc\"]*100, 2),\n",
        "        \"test_acc\": round(test_acc*100, 2),\n",
        "        \"trainable_params_m\": round(sum(p.numel() for p in trainable_params(model))/1e6, 2),\n",
        "    }\n",
        "\n",
        "summary = []\n",
        "summary.append(summarize(\"E1_head_only\", best_e1, model_e1))\n",
        "summary.append(summarize(\"E2_layer4+head\", best_e2, model_e2))\n",
        "summary.append(summarize(\"E3_full_pretrained\", best_e3, model_e3))\n",
        "summary.append(summarize(\"E4_full_scratch\", best_e4, model_e4))\n",
        "\n",
        "import pandas as pd\n",
        "pd.DataFrame(summary)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bKep-wQW8-O_"
      },
      "outputs": [],
      "source": [
        "import time, torch\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def count_trainable_params(m):\n",
        "    return sum(p.numel() for p in m.parameters() if p.requires_grad)\n",
        "\n",
        "@torch.no_grad()\n",
        "def eval_test_acc(model, loader):\n",
        "    model.eval()\n",
        "    crit = torch.nn.CrossEntropyLoss()\n",
        "    tot_acc, tot_n = 0.0, 0\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
        "        logits = model(x)\n",
        "        preds = logits.argmax(1)\n",
        "        tot_acc += (preds == y).float().sum().item()\n",
        "        tot_n   += y.size(0)\n",
        "    return 100.0 * tot_acc / tot_n\n",
        "\n",
        "# pull best epochs (already stored by your fit)\n",
        "def best_epoch_of(best_dict):\n",
        "    return best_dict.get(\"epoch\", None) if isinstance(best_dict, dict) else None\n",
        "\n",
        "summary_rows = []\n",
        "for tag, model, best in [\n",
        "    (\"E1_head_only\",        model_e1, best_e1),\n",
        "    (\"E2_layer4+head\",      model_e2, best_e2),\n",
        "    (\"E3_full_pretrained\",  model_e3, best_e3),\n",
        "    (\"E4_full_scratch\",     model_e4, best_e4),\n",
        "]:\n",
        "    be = best_epoch_of(best)\n",
        "    test_acc = eval_test_acc(model, test_loader)\n",
        "    params_m = count_trainable_params(model)/1e6\n",
        "    summary_rows.append({\n",
        "        \"exp\": tag,\n",
        "        \"best_epoch\": be,\n",
        "        \"test_acc_%\": round(test_acc, 2),\n",
        "        \"trainable_params_M\": round(params_m, 2),\n",
        "        \"steps_to_best\": None,       # we'll fill after we estimate per-step time\n",
        "        \"est_step_time_s\": None,     # ditto\n",
        "        \"est_time_to_best_min\": None # ditto\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(summary_rows)\n",
        "df\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "adkb18v99Bh9"
      },
      "outputs": [],
      "source": [
        "def estimate_step_time_seconds(model, loader, batches=5):\n",
        "    model = model.to(DEVICE)\n",
        "    model.train()\n",
        "    # tiny optimizer for timing (won't be used for real training)\n",
        "    opt = torch.optim.SGD((p for p in model.parameters() if p.requires_grad), lr=1e-3, momentum=0.0)\n",
        "    crit = torch.nn.CrossEntropyLoss()\n",
        "\n",
        "    n = 0\n",
        "    start = None\n",
        "    # warmup one batch for stable timing\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        out = model(x)\n",
        "        loss = crit(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        break\n",
        "\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "\n",
        "    t0 = time.time()\n",
        "    for x, y in loader:\n",
        "        x, y = x.to(DEVICE, non_blocking=True), torch.tensor(y, device=DEVICE)\n",
        "        opt.zero_grad(set_to_none=True)\n",
        "        out = model(x)\n",
        "        loss = crit(out, y)\n",
        "        loss.backward()\n",
        "        opt.step()\n",
        "        n += 1\n",
        "        if n >= batches:\n",
        "            break\n",
        "\n",
        "    if DEVICE.type == 'cuda':\n",
        "        torch.cuda.synchronize()\n",
        "    t1 = time.time()\n",
        "\n",
        "    return (t1 - t0) / max(1, n)\n",
        "\n",
        "# fill in steps_to_best (best_epoch * steps/epoch) and estimated time_to_best\n",
        "steps_per_epoch = len(train_loader)\n",
        "\n",
        "for i in range(len(df)):\n",
        "    tag = df.loc[i, \"exp\"]\n",
        "    model = {\"E1_head_only\": model_e1, \"E2_layer4+head\": model_e2,\n",
        "             \"E3_full_pretrained\": model_e3, \"E4_full_scratch\": model_e4}[tag]\n",
        "    be = df.loc[i, \"best_epoch\"]\n",
        "    if be is None or be <= 0:\n",
        "        continue\n",
        "    step_time = estimate_step_time_seconds(model, train_loader, batches=5)\n",
        "    steps_to_best = be * steps_per_epoch\n",
        "    time_to_best_min = (step_time * steps_to_best) / 60.0\n",
        "    df.loc[i, \"steps_to_best\"] = int(steps_to_best)\n",
        "    df.loc[i, \"est_step_time_s\"] = round(step_time, 3)\n",
        "    df.loc[i, \"est_time_to_best_min\"] = round(time_to_best_min, 2)\n",
        "\n",
        "df.sort_values([\"test_acc_%\", \"est_time_to_best_min\"], ascending=[False, True])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "94zzq7ct9DFF"
      },
      "outputs": [],
      "source": [
        "# Accuracy vs estimated time-to-best\n",
        "plt.figure()\n",
        "valid = df.dropna(subset=[\"est_time_to_best_min\"])\n",
        "plt.scatter(valid[\"est_time_to_best_min\"], valid[\"test_acc_%\"])\n",
        "for _, row in valid.iterrows():\n",
        "    plt.annotate(row[\"exp\"], (row[\"est_time_to_best_min\"], row[\"test_acc_%\"]))\n",
        "plt.xlabel(\"Estimated time to best (min)\")\n",
        "plt.ylabel(\"Test accuracy (%)\")\n",
        "plt.title(\"Compute vs Accuracy (post-hoc estimate)\")\n",
        "plt.show()\n",
        "\n",
        "# Accuracy vs trainable params (proxy for adaptation budget)\n",
        "plt.figure()\n",
        "plt.scatter(df[\"trainable_params_M\"], df[\"test_acc_%\"])\n",
        "for _, row in df.iterrows():\n",
        "    plt.annotate(row[\"exp\"], (row[\"trainable_params_M\"], row[\"test_acc_%\"]))\n",
        "plt.xlabel(\"Trainable parameters (Millions)\")\n",
        "plt.ylabel(\"Test accuracy (%)\")\n",
        "plt.title(\"Accuracy vs Adaptation Budget\")\n",
        "plt.show()\n",
        "\n",
        "df\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}